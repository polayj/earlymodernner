# =========================================================================
# EarlyModernNER Ensemble Training - PERSON Specialist (AUGMENTED DATA)
# Model: Qwen3-4B-Instruct with QLoRA
# Includes synthetic negatives and positives for better precision/recall
# =========================================================================

# ========= Model & data =========
base_model_name: "Qwen/Qwen3-4B-Instruct-2507"
train_file: "data/ensemble_training_filtered/person/train_chat_augmented.jsonl"
eval_file: "data/ensemble_training_with_silver/person/dev_chat.jsonl"
output_dir: "outputs/ensemble/person_lora_augmented"

# ========= Sequence / batching =========
max_seq_length: 2048
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4      # 2 * 4 = effective batch 8
eval_accumulation_steps: 1

# ========= Training schedule =========
num_train_epochs: 2
learning_rate: 2.0e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.01
max_grad_norm: 1.0

# ========= Precision & memory (QLoRA) =========
use_4bit: true
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true

gradient_checkpointing: true
tf32: true

# ========= LoRA config =========
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# ========= Logging & eval =========
logging_steps: 10
save_steps: 200
eval_steps: 200
save_total_limit: 3
evaluation_strategy: "steps"
do_eval: true

# ========= Misc =========
seed: 42
report_to:
  - "none"
