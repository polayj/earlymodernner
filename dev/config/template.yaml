# =========================================================================
# EarlyModernNER Training Configuration Template
#
# Copy this file and customize for your entity type.
# See the ensemble_*.yaml files for examples of working configurations.
# =========================================================================

# ========= Model & Data =========
# Base model to fine-tune (Qwen 4B works well for this task)
base_model_name: "Qwen/Qwen3-4B-Instruct-2507"

# Training data in chat format (JSONL with messages array)
train_file: "path/to/your/train_chat.jsonl"
eval_file: "path/to/your/dev_chat.jsonl"

# Where to save the trained adapter
output_dir: "outputs/your_entity_lora"

# ========= Sequence & Batching =========
max_seq_length: 2048
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4      # Effective batch = 2 * 4 = 8

# ========= Training Schedule =========
num_train_epochs: 2                 # 2-6 epochs typically works well
learning_rate: 2.0e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.01
max_grad_norm: 1.0

# ========= Quantization (QLoRA) =========
use_4bit: true
bnb_4bit_compute_dtype: "bfloat16"
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true

gradient_checkpointing: true
tf32: true

# ========= LoRA Configuration =========
lora_r: 64                          # Rank (higher = more capacity)
lora_alpha: 16                      # Scaling factor
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# ========= Logging & Checkpoints =========
logging_steps: 10
save_steps: 200
eval_steps: 200
save_total_limit: 3
evaluation_strategy: "steps"
do_eval: true

# ========= Misc =========
seed: 42
report_to:
  - "none"                          # Options: "none", "wandb", "tensorboard"
